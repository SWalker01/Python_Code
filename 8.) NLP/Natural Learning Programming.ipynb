{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.4.5)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "#nltk.org\n",
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "print(brown.categories())\n",
    "# from this i will extract the news category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=brown.sents(categories=\"news\") #data is corpus and it has documents(sentences)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.sents(categories=\"news\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4623"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data) #it has this len document and related with news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Fulton',\n",
       " 'County',\n",
       " 'Grand',\n",
       " 'Jury',\n",
       " 'said',\n",
       " 'Friday',\n",
       " 'an',\n",
       " 'investigation',\n",
       " 'of',\n",
       " \"Atlanta's\",\n",
       " 'recent',\n",
       " 'primary',\n",
       " 'election',\n",
       " 'produced',\n",
       " '``',\n",
       " 'no',\n",
       " 'evidence',\n",
       " \"''\",\n",
       " 'that',\n",
       " 'any',\n",
       " 'irregularities',\n",
       " 'took',\n",
       " 'place',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0] # it is dicument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'main',\n",
       " 'reasons',\n",
       " 'for',\n",
       " 'National',\n",
       " 'Library',\n",
       " 'Week',\n",
       " ',',\n",
       " 'April',\n",
       " '16-22',\n",
       " ',',\n",
       " 'and',\n",
       " 'for',\n",
       " 'its',\n",
       " 'theme',\n",
       " ':',\n",
       " '``',\n",
       " 'For',\n",
       " 'a',\n",
       " 'richer',\n",
       " ',',\n",
       " 'fuller',\n",
       " 'life',\n",
       " ',',\n",
       " 'read',\n",
       " \"''\",\n",
       " '!',\n",
       " '!']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[4622]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Implementation of Georgia's automobile title law was also recommended by the outgoing jury .\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(data[11])\n",
    "#they are giving docs in break format we have just joined it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words(categories='news')\n",
    "#it has lots of words in documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_words= brown.sents(categories='news')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4623"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP pipeline\n",
    "- Data Collection\n",
    "- tokenization,stopword removal,stemming\n",
    "- building vocabulary[BOW]\n",
    "- vecorization of all docs\n",
    "- perform classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = \"I love my country. I love to play cricket\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'love', 'my', 'country.', 'I', 'love', 'to', 'play', 'cricket']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize ,word_tokenize\n",
    "#Data give in Doc also tokenize\n",
    "#sent tokenize and word split by word and split by sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\ARYAN\n",
      "[nltk_data]     GULATI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"\"\"It was a very pleasant day. The weather was cool and there were light showers. \n",
    "I went to the market to buy some fruits.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'was', 'a', 'very', 'pleasant', 'day', '.', 'The', 'weather', 'was', 'cool', 'and', 'there', 'were', 'light', 'showers', '.', 'I', 'went', 'to', 'the', 'market', 'to', 'buy', 'some', 'fruits', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(document))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It was a very pleasant day.',\n",
       " 'The weather was cool and there were light showers.',\n",
       " 'I went to the market to buy some fruits.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It was a very pleasant day',\n",
       " ' The weather was cool and there were light showers',\n",
       " ' \\nI went to the market to buy some fruits',\n",
       " '']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document.split(\".\")\n",
    "#it include \\n and so sent tokenize are avance version and have diff techinque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"Mr. Modi is a good man. He is indian.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr. Modi is a good man.', 'He is indian.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr.', 'Modi', 'is', 'a', 'good', 'man', '.', 'He', 'is', 'indian', '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr', ' Modi is a good man', ' He is indian', '']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.split(\".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwards: Package 'stopwards' not found in\n",
      "[nltk_data]     index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwards\")\n",
    "#orwe can search in ntlk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=stopwords.words('english')\n",
    "#its just a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'not' in l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.remove(\"not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'not' in l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['إذ',\n",
       " 'إذا',\n",
       " 'إذما',\n",
       " 'إذن',\n",
       " 'أف',\n",
       " 'أقل',\n",
       " 'أكثر',\n",
       " 'ألا',\n",
       " 'إلا',\n",
       " 'التي',\n",
       " 'الذي',\n",
       " 'الذين',\n",
       " 'اللاتي',\n",
       " 'اللائي',\n",
       " 'اللتان',\n",
       " 'اللتيا',\n",
       " 'اللتين',\n",
       " 'اللذان',\n",
       " 'اللذين',\n",
       " 'اللواتي',\n",
       " 'إلى',\n",
       " 'إليك',\n",
       " 'إليكم',\n",
       " 'إليكما',\n",
       " 'إليكن',\n",
       " 'أم',\n",
       " 'أما',\n",
       " 'أما',\n",
       " 'إما',\n",
       " 'أن',\n",
       " 'إن',\n",
       " 'إنا',\n",
       " 'أنا',\n",
       " 'أنت',\n",
       " 'أنتم',\n",
       " 'أنتما',\n",
       " 'أنتن',\n",
       " 'إنما',\n",
       " 'إنه',\n",
       " 'أنى',\n",
       " 'أنى',\n",
       " 'آه',\n",
       " 'آها',\n",
       " 'أو',\n",
       " 'أولاء',\n",
       " 'أولئك',\n",
       " 'أوه',\n",
       " 'آي',\n",
       " 'أي',\n",
       " 'أيها',\n",
       " 'إي',\n",
       " 'أين',\n",
       " 'أين',\n",
       " 'أينما',\n",
       " 'إيه',\n",
       " 'بخ',\n",
       " 'بس',\n",
       " 'بعد',\n",
       " 'بعض',\n",
       " 'بك',\n",
       " 'بكم',\n",
       " 'بكم',\n",
       " 'بكما',\n",
       " 'بكن',\n",
       " 'بل',\n",
       " 'بلى',\n",
       " 'بما',\n",
       " 'بماذا',\n",
       " 'بمن',\n",
       " 'بنا',\n",
       " 'به',\n",
       " 'بها',\n",
       " 'بهم',\n",
       " 'بهما',\n",
       " 'بهن',\n",
       " 'بي',\n",
       " 'بين',\n",
       " 'بيد',\n",
       " 'تلك',\n",
       " 'تلكم',\n",
       " 'تلكما',\n",
       " 'ته',\n",
       " 'تي',\n",
       " 'تين',\n",
       " 'تينك',\n",
       " 'ثم',\n",
       " 'ثمة',\n",
       " 'حاشا',\n",
       " 'حبذا',\n",
       " 'حتى',\n",
       " 'حيث',\n",
       " 'حيثما',\n",
       " 'حين',\n",
       " 'خلا',\n",
       " 'دون',\n",
       " 'ذا',\n",
       " 'ذات',\n",
       " 'ذاك',\n",
       " 'ذان',\n",
       " 'ذانك',\n",
       " 'ذلك',\n",
       " 'ذلكم',\n",
       " 'ذلكما',\n",
       " 'ذلكن',\n",
       " 'ذه',\n",
       " 'ذو',\n",
       " 'ذوا',\n",
       " 'ذواتا',\n",
       " 'ذواتي',\n",
       " 'ذي',\n",
       " 'ذين',\n",
       " 'ذينك',\n",
       " 'ريث',\n",
       " 'سوف',\n",
       " 'سوى',\n",
       " 'شتان',\n",
       " 'عدا',\n",
       " 'عسى',\n",
       " 'عل',\n",
       " 'على',\n",
       " 'عليك',\n",
       " 'عليه',\n",
       " 'عما',\n",
       " 'عن',\n",
       " 'عند',\n",
       " 'غير',\n",
       " 'فإذا',\n",
       " 'فإن',\n",
       " 'فلا',\n",
       " 'فمن',\n",
       " 'في',\n",
       " 'فيم',\n",
       " 'فيما',\n",
       " 'فيه',\n",
       " 'فيها',\n",
       " 'قد',\n",
       " 'كأن',\n",
       " 'كأنما',\n",
       " 'كأي',\n",
       " 'كأين',\n",
       " 'كذا',\n",
       " 'كذلك',\n",
       " 'كل',\n",
       " 'كلا',\n",
       " 'كلاهما',\n",
       " 'كلتا',\n",
       " 'كلما',\n",
       " 'كليكما',\n",
       " 'كليهما',\n",
       " 'كم',\n",
       " 'كم',\n",
       " 'كما',\n",
       " 'كي',\n",
       " 'كيت',\n",
       " 'كيف',\n",
       " 'كيفما',\n",
       " 'لا',\n",
       " 'لاسيما',\n",
       " 'لدى',\n",
       " 'لست',\n",
       " 'لستم',\n",
       " 'لستما',\n",
       " 'لستن',\n",
       " 'لسن',\n",
       " 'لسنا',\n",
       " 'لعل',\n",
       " 'لك',\n",
       " 'لكم',\n",
       " 'لكما',\n",
       " 'لكن',\n",
       " 'لكنما',\n",
       " 'لكي',\n",
       " 'لكيلا',\n",
       " 'لم',\n",
       " 'لما',\n",
       " 'لن',\n",
       " 'لنا',\n",
       " 'له',\n",
       " 'لها',\n",
       " 'لهم',\n",
       " 'لهما',\n",
       " 'لهن',\n",
       " 'لو',\n",
       " 'لولا',\n",
       " 'لوما',\n",
       " 'لي',\n",
       " 'لئن',\n",
       " 'ليت',\n",
       " 'ليس',\n",
       " 'ليسا',\n",
       " 'ليست',\n",
       " 'ليستا',\n",
       " 'ليسوا',\n",
       " 'ما',\n",
       " 'ماذا',\n",
       " 'متى',\n",
       " 'مذ',\n",
       " 'مع',\n",
       " 'مما',\n",
       " 'ممن',\n",
       " 'من',\n",
       " 'منه',\n",
       " 'منها',\n",
       " 'منذ',\n",
       " 'مه',\n",
       " 'مهما',\n",
       " 'نحن',\n",
       " 'نحو',\n",
       " 'نعم',\n",
       " 'ها',\n",
       " 'هاتان',\n",
       " 'هاته',\n",
       " 'هاتي',\n",
       " 'هاتين',\n",
       " 'هاك',\n",
       " 'هاهنا',\n",
       " 'هذا',\n",
       " 'هذان',\n",
       " 'هذه',\n",
       " 'هذي',\n",
       " 'هذين',\n",
       " 'هكذا',\n",
       " 'هل',\n",
       " 'هلا',\n",
       " 'هم',\n",
       " 'هما',\n",
       " 'هن',\n",
       " 'هنا',\n",
       " 'هناك',\n",
       " 'هنالك',\n",
       " 'هو',\n",
       " 'هؤلاء',\n",
       " 'هي',\n",
       " 'هيا',\n",
       " 'هيت',\n",
       " 'هيهات',\n",
       " 'والذي',\n",
       " 'والذين',\n",
       " 'وإذ',\n",
       " 'وإذا',\n",
       " 'وإن',\n",
       " 'ولا',\n",
       " 'ولكن',\n",
       " 'ولو',\n",
       " 'وما',\n",
       " 'ومن',\n",
       " 'وهو',\n",
       " 'يا',\n",
       " 'a',\n",
       " 'ad',\n",
       " 'altı',\n",
       " 'altmış',\n",
       " 'amma',\n",
       " 'arasında',\n",
       " 'artıq',\n",
       " 'ay',\n",
       " 'az',\n",
       " 'bax',\n",
       " 'belə',\n",
       " 'bəli',\n",
       " 'bəlkə',\n",
       " 'beş',\n",
       " 'bəy',\n",
       " 'bəzən',\n",
       " 'bəzi',\n",
       " 'bilər',\n",
       " 'bir',\n",
       " 'biraz',\n",
       " 'biri',\n",
       " 'birşey',\n",
       " 'biz',\n",
       " 'bizim',\n",
       " 'bizlər',\n",
       " 'bu',\n",
       " 'buna',\n",
       " 'bundan',\n",
       " 'bunların',\n",
       " 'bunu',\n",
       " 'bunun',\n",
       " 'buradan',\n",
       " 'bütün',\n",
       " 'ci',\n",
       " 'cı',\n",
       " 'çox',\n",
       " 'cu',\n",
       " 'cü',\n",
       " 'çünki',\n",
       " 'da',\n",
       " 'daha',\n",
       " 'də',\n",
       " 'dedi',\n",
       " 'dək',\n",
       " 'dən',\n",
       " 'dəqiqə',\n",
       " 'deyil',\n",
       " 'dir',\n",
       " 'doqquz',\n",
       " 'doqsan',\n",
       " 'dörd',\n",
       " 'düz',\n",
       " 'ə',\n",
       " 'edən',\n",
       " 'edir',\n",
       " 'əgər',\n",
       " 'əlbəttə',\n",
       " 'elə',\n",
       " 'əlli',\n",
       " 'ən',\n",
       " 'əslində',\n",
       " 'et',\n",
       " 'etdi',\n",
       " 'etmə',\n",
       " 'etmək',\n",
       " 'faiz',\n",
       " 'gilə',\n",
       " 'görə',\n",
       " 'ha',\n",
       " 'haqqında',\n",
       " 'harada',\n",
       " 'hə',\n",
       " 'heç',\n",
       " 'həm',\n",
       " 'həmin',\n",
       " 'həmişə',\n",
       " 'hər',\n",
       " 'ı',\n",
       " 'idi',\n",
       " 'iki',\n",
       " 'il',\n",
       " 'ildə',\n",
       " 'ilə',\n",
       " 'ilk',\n",
       " 'in',\n",
       " 'indi',\n",
       " 'isə',\n",
       " 'istifadə',\n",
       " 'iyirmi',\n",
       " 'ki',\n",
       " 'kim',\n",
       " 'kimə',\n",
       " 'kimi',\n",
       " 'lakin',\n",
       " 'lap',\n",
       " 'məhz',\n",
       " 'mən',\n",
       " 'mənə',\n",
       " 'mirşey',\n",
       " 'nə',\n",
       " 'nəhayət',\n",
       " 'niyə',\n",
       " 'o',\n",
       " 'obirisi',\n",
       " 'of',\n",
       " 'olan',\n",
       " 'olar',\n",
       " 'olaraq',\n",
       " 'oldu',\n",
       " 'olduğu',\n",
       " 'olmadı',\n",
       " 'olmaz',\n",
       " 'olmuşdur',\n",
       " 'olsun',\n",
       " 'olur',\n",
       " 'on',\n",
       " 'ona',\n",
       " 'ondan',\n",
       " 'onlar',\n",
       " 'onlardan',\n",
       " 'onların ',\n",
       " 'onsuzda',\n",
       " 'onu',\n",
       " 'onun',\n",
       " 'oradan',\n",
       " 'otuz',\n",
       " 'öz',\n",
       " 'özü',\n",
       " 'qarşı',\n",
       " 'qədər',\n",
       " 'qırx',\n",
       " 'saat',\n",
       " 'sadəcə',\n",
       " 'saniyə',\n",
       " 'səhv',\n",
       " 'səkkiz',\n",
       " 'səksən',\n",
       " 'sən',\n",
       " 'sənə',\n",
       " 'sənin',\n",
       " 'siz',\n",
       " 'sizin',\n",
       " 'sizlər',\n",
       " 'sonra',\n",
       " 'təəssüf',\n",
       " 'ü',\n",
       " 'üç',\n",
       " 'üçün',\n",
       " 'var',\n",
       " 'və',\n",
       " 'xan',\n",
       " 'xanım',\n",
       " 'xeyr',\n",
       " 'ya',\n",
       " 'yalnız',\n",
       " 'yaxşı',\n",
       " 'yeddi',\n",
       " 'yenə',\n",
       " 'yəni',\n",
       " 'yetmiş',\n",
       " 'yox',\n",
       " 'yoxdur',\n",
       " 'yoxsa',\n",
       " 'yüz',\n",
       " 'zamanog',\n",
       " 'i',\n",
       " 'jeg',\n",
       " 'det',\n",
       " 'at',\n",
       " 'en',\n",
       " 'den',\n",
       " 'til',\n",
       " 'er',\n",
       " 'som',\n",
       " 'på',\n",
       " 'de',\n",
       " 'med',\n",
       " 'han',\n",
       " 'af',\n",
       " 'for',\n",
       " 'ikke',\n",
       " 'der',\n",
       " 'var',\n",
       " 'mig',\n",
       " 'sig',\n",
       " 'men',\n",
       " 'et',\n",
       " 'har',\n",
       " 'om',\n",
       " 'vi',\n",
       " 'min',\n",
       " 'havde',\n",
       " 'ham',\n",
       " 'hun',\n",
       " 'nu',\n",
       " 'over',\n",
       " 'da',\n",
       " 'fra',\n",
       " 'du',\n",
       " 'ud',\n",
       " 'sin',\n",
       " 'dem',\n",
       " 'os',\n",
       " 'op',\n",
       " 'man',\n",
       " 'hans',\n",
       " 'hvor',\n",
       " 'eller',\n",
       " 'hvad',\n",
       " 'skal',\n",
       " 'selv',\n",
       " 'her',\n",
       " 'alle',\n",
       " 'vil',\n",
       " 'blev',\n",
       " 'kunne',\n",
       " 'ind',\n",
       " 'når',\n",
       " 'være',\n",
       " 'dog',\n",
       " 'noget',\n",
       " 'ville',\n",
       " 'jo',\n",
       " 'deres',\n",
       " 'efter',\n",
       " 'ned',\n",
       " 'skulle',\n",
       " 'denne',\n",
       " 'end',\n",
       " 'dette',\n",
       " 'mit',\n",
       " 'også',\n",
       " 'under',\n",
       " 'have',\n",
       " 'dig',\n",
       " 'anden',\n",
       " 'hende',\n",
       " 'mine',\n",
       " 'alt',\n",
       " 'meget',\n",
       " 'sit',\n",
       " 'sine',\n",
       " 'vor',\n",
       " 'mod',\n",
       " 'disse',\n",
       " 'hvis',\n",
       " 'din',\n",
       " 'nogle',\n",
       " 'hos',\n",
       " 'blive',\n",
       " 'mange',\n",
       " 'ad',\n",
       " 'bliver',\n",
       " 'hendes',\n",
       " 'været',\n",
       " 'thi',\n",
       " 'jer',\n",
       " 'sådan',\n",
       " 'de',\n",
       " 'en',\n",
       " 'van',\n",
       " 'ik',\n",
       " 'te',\n",
       " 'dat',\n",
       " 'die',\n",
       " 'in',\n",
       " 'een',\n",
       " 'hij',\n",
       " 'het',\n",
       " 'niet',\n",
       " 'zijn',\n",
       " 'is',\n",
       " 'was',\n",
       " 'op',\n",
       " 'aan',\n",
       " 'met',\n",
       " 'als',\n",
       " 'voor',\n",
       " 'had',\n",
       " 'er',\n",
       " 'maar',\n",
       " 'om',\n",
       " 'hem',\n",
       " 'dan',\n",
       " 'zou',\n",
       " 'of',\n",
       " 'wat',\n",
       " 'mijn',\n",
       " 'men',\n",
       " 'dit',\n",
       " 'zo',\n",
       " 'door',\n",
       " 'over',\n",
       " 'ze',\n",
       " 'zich',\n",
       " 'bij',\n",
       " 'ook',\n",
       " 'tot',\n",
       " 'je',\n",
       " 'mij',\n",
       " 'uit',\n",
       " 'der',\n",
       " 'daar',\n",
       " 'haar',\n",
       " 'naar',\n",
       " 'heb',\n",
       " 'hoe',\n",
       " 'heeft',\n",
       " 'hebben',\n",
       " 'deze',\n",
       " 'u',\n",
       " 'want',\n",
       " 'nog',\n",
       " 'zal',\n",
       " 'me',\n",
       " 'zij',\n",
       " 'nu',\n",
       " 'ge',\n",
       " 'geen',\n",
       " 'omdat',\n",
       " 'iets',\n",
       " 'worden',\n",
       " 'toch',\n",
       " 'al',\n",
       " 'waren',\n",
       " 'veel',\n",
       " 'meer',\n",
       " 'doen',\n",
       " 'toen',\n",
       " 'moet',\n",
       " 'ben',\n",
       " 'zonder',\n",
       " 'kan',\n",
       " 'hun',\n",
       " 'dus',\n",
       " 'alles',\n",
       " 'onder',\n",
       " 'ja',\n",
       " 'eens',\n",
       " 'hier',\n",
       " 'wie',\n",
       " 'werd',\n",
       " 'altijd',\n",
       " 'doch',\n",
       " 'wordt',\n",
       " 'wezen',\n",
       " 'kunnen',\n",
       " 'ons',\n",
       " 'zelf',\n",
       " 'tegen',\n",
       " 'na',\n",
       " 'reeds',\n",
       " 'wil',\n",
       " 'kon',\n",
       " 'niets',\n",
       " 'uw',\n",
       " 'iemand',\n",
       " 'geweest',\n",
       " 'andere',\n",
       " 'i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'olla',\n",
       " 'olen',\n",
       " 'olet',\n",
       " 'on',\n",
       " 'olemme',\n",
       " 'olette',\n",
       " 'ovat',\n",
       " 'ole',\n",
       " 'oli',\n",
       " 'olisi',\n",
       " 'olisit',\n",
       " 'olisin',\n",
       " 'olisimme',\n",
       " 'olisitte',\n",
       " 'olisivat',\n",
       " 'olit',\n",
       " 'olin',\n",
       " 'olimme',\n",
       " 'olitte',\n",
       " 'olivat',\n",
       " 'ollut',\n",
       " 'olleet',\n",
       " 'en',\n",
       " 'et',\n",
       " 'ei',\n",
       " 'emme',\n",
       " 'ette',\n",
       " 'eivät',\n",
       " 'minä',\n",
       " 'minun',\n",
       " 'minut',\n",
       " 'minua',\n",
       " 'minussa',\n",
       " 'minusta',\n",
       " 'minuun',\n",
       " 'minulla',\n",
       " 'minulta',\n",
       " 'minulle',\n",
       " 'sinä',\n",
       " 'sinun',\n",
       " 'sinut',\n",
       " 'sinua',\n",
       " 'sinussa',\n",
       " 'sinusta',\n",
       " 'sinuun',\n",
       " 'sinulla',\n",
       " 'sinulta',\n",
       " 'sinulle',\n",
       " 'hän',\n",
       " 'hänen',\n",
       " 'hänet',\n",
       " 'häntä',\n",
       " 'hänessä',\n",
       " 'hänestä',\n",
       " 'häneen',\n",
       " 'hänellä',\n",
       " 'häneltä',\n",
       " 'hänelle',\n",
       " 'me',\n",
       " 'meidän',\n",
       " 'meidät',\n",
       " 'meitä',\n",
       " 'meissä',\n",
       " 'meistä',\n",
       " 'meihin',\n",
       " 'meillä',\n",
       " 'meiltä',\n",
       " 'meille',\n",
       " 'te',\n",
       " 'teidän',\n",
       " 'teidät',\n",
       " 'teitä',\n",
       " 'teissä',\n",
       " 'teistä',\n",
       " 'teihin',\n",
       " 'teillä',\n",
       " 'teiltä',\n",
       " 'teille',\n",
       " 'he',\n",
       " 'heidän',\n",
       " 'heidät',\n",
       " 'heitä',\n",
       " 'heissä',\n",
       " 'heistä',\n",
       " 'heihin',\n",
       " 'heillä',\n",
       " 'heiltä',\n",
       " 'heille',\n",
       " 'tämä',\n",
       " 'tämän',\n",
       " 'tätä',\n",
       " 'tässä',\n",
       " 'tästä',\n",
       " 'tähän',\n",
       " 'tallä',\n",
       " 'tältä',\n",
       " 'tälle',\n",
       " 'tänä',\n",
       " 'täksi',\n",
       " 'tuo',\n",
       " 'tuon',\n",
       " 'tuotä',\n",
       " 'tuossa',\n",
       " 'tuosta',\n",
       " 'tuohon',\n",
       " 'tuolla',\n",
       " 'tuolta',\n",
       " 'tuolle',\n",
       " 'tuona',\n",
       " 'tuoksi',\n",
       " 'se',\n",
       " 'sen',\n",
       " 'sitä',\n",
       " 'siinä',\n",
       " 'siitä',\n",
       " 'siihen',\n",
       " 'sillä',\n",
       " 'siltä',\n",
       " 'sille',\n",
       " 'sinä',\n",
       " 'siksi',\n",
       " 'nämä',\n",
       " 'näiden',\n",
       " 'näitä',\n",
       " 'näissä',\n",
       " 'näistä',\n",
       " 'näihin',\n",
       " 'näillä',\n",
       " 'näiltä',\n",
       " 'näille',\n",
       " 'näinä',\n",
       " 'näiksi',\n",
       " 'nuo',\n",
       " 'noiden',\n",
       " 'noita',\n",
       " 'noissa',\n",
       " 'noista',\n",
       " 'noihin',\n",
       " 'noilla',\n",
       " 'noilta',\n",
       " 'noille',\n",
       " 'noina',\n",
       " 'noiksi',\n",
       " 'ne',\n",
       " 'niiden',\n",
       " 'niitä',\n",
       " 'niissä',\n",
       " 'niistä',\n",
       " 'niihin',\n",
       " 'niillä',\n",
       " 'niiltä',\n",
       " 'niille',\n",
       " 'niinä',\n",
       " 'niiksi',\n",
       " 'kuka',\n",
       " 'kenen',\n",
       " 'kenet',\n",
       " 'ketä',\n",
       " 'kenessä',\n",
       " 'kenestä',\n",
       " 'keneen',\n",
       " 'kenellä',\n",
       " 'keneltä',\n",
       " 'kenelle',\n",
       " 'kenenä',\n",
       " 'keneksi',\n",
       " 'ketkä',\n",
       " 'keiden',\n",
       " 'ketkä',\n",
       " 'keitä',\n",
       " 'keissä',\n",
       " 'keistä',\n",
       " 'keihin',\n",
       " 'keillä',\n",
       " 'keiltä',\n",
       " 'keille',\n",
       " 'keinä',\n",
       " 'keiksi',\n",
       " 'mikä',\n",
       " 'minkä',\n",
       " 'minkä',\n",
       " 'mitä',\n",
       " 'missä',\n",
       " 'mistä',\n",
       " 'mihin',\n",
       " 'millä',\n",
       " 'miltä',\n",
       " 'mille',\n",
       " 'minä',\n",
       " 'miksi',\n",
       " 'mitkä',\n",
       " 'joka',\n",
       " 'jonka',\n",
       " 'jota',\n",
       " 'jossa',\n",
       " 'josta',\n",
       " 'johon',\n",
       " 'jolla',\n",
       " 'jolta',\n",
       " 'jolle',\n",
       " 'jona',\n",
       " 'joksi',\n",
       " 'jotka',\n",
       " 'joiden',\n",
       " 'joita',\n",
       " 'joissa',\n",
       " 'joista',\n",
       " 'joihin',\n",
       " 'joilla',\n",
       " 'joilta',\n",
       " 'joille',\n",
       " 'joina',\n",
       " 'joiksi',\n",
       " 'että',\n",
       " ...]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words()\n",
    "#https://www.ranks.nl/stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'not' in sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pleasant',\n",
       " 'day',\n",
       " '.',\n",
       " 'weather',\n",
       " 'cool',\n",
       " 'light',\n",
       " 'showers',\n",
       " '.',\n",
       " 'went',\n",
       " 'market',\n",
       " 'buy',\n",
       " 'fruits',\n",
       " '.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in word_tokenize(document.lower()) if w not in l] # list comprehension\n",
    "#its just saying for  w in word document if w not in l then print w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It pleasant day . The weather cool light showers . I went market buy fruits .'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([w for w in word_tokenize(document) if w not in l]) # list comprehension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pleasant day . weather cool light showers . went market buy fruits .'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([w for w in word_tokenize(document.lower()) if w not in l]) # list comprehension\n",
    "#we have use here lower function here \n",
    "#hence preventing overfitting\n",
    "#so lower makes all words same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer#this is class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://regex101.com/\n",
    "#https://www.regexpal.com/\n",
    "#https://www.rexegg.com/regex-quickstart.html\n",
    "#https://www.rexegg.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_regex_tokenizer = RegexpTokenizer(pattern=\"[a-z]+\")\n",
    "#like sent _token.. and word its like them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = \"Foxes love to make jumps. The quick brown fox 76989i7yy 876908 %^^%$&*** was seen jumping over the lovely dog from a 6ft feet,,  high wall...... mohituniyal@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Foxes love to make jumps. The quick brown fox 76989i7yy 876908 %^^%$&*** was seen jumping over the lovely dog from a 6ft feet,,  high wall...... mohituniyal@gmail.com'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'foxes love to make jumps the quick brown fox i yy was seen jumping over the lovely dog from a ft feet high wall mohituniyal gmail com'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(my_regex_tokenizer.tokenize(abc.lower()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it',\n",
       " 'was',\n",
       " 'a',\n",
       " 'very',\n",
       " 'pleasant',\n",
       " 'day',\n",
       " 'the',\n",
       " 'weather',\n",
       " 'was',\n",
       " 'cool',\n",
       " 'and',\n",
       " 'there',\n",
       " 'were',\n",
       " 'light',\n",
       " 'showers',\n",
       " 'i',\n",
       " 'went',\n",
       " 'to',\n",
       " 'the',\n",
       " 'market',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'some',\n",
       " 'fruits']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_regex_tokenizer.tokenize(document.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "#these are different type of stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb = SnowballStemmer(language='english')\n",
    "#word that can be made from root "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb.stem('lovely')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb.stem(\"loved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb.stem(\"running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ran'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb.stem(\"ran\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'movi'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb.stem('movie')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'microsoft'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb.stem(\"microsoft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'googl'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb.stem(\"google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idea is to reduce feature \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus is list of documents\n",
    "corpus = [\n",
    "        'Indian cricket team will wins World Cup, says Capt. Virat Kohli. World cup will be held at Sri Lanka.',\n",
    "        'We will win next Lok Sabha Elections, says confident Indian PM',\n",
    "        'Former Indian president APJ Abdul Kalam won the hearts of the people.',\n",
    "        'The movie Raazi is an exciting Indian Spy thriller based upon a real story.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have to iterate and see unique words \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer() #constructor\n",
    "#cv is count vectorize  ka object\n",
    "\n",
    "#way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode\n",
    "#new documents using that vocabulary. ... Call the fit() function in order to learn a vocabulary from one or more documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv #cv is CountVectorizer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x45 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 51 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit_transform(corpus) #raw_documents pass as a parameter\n",
    "\n",
    "#Learn the vocabulary dictionary and return term-document matrix.\n",
    "#it will do training and apply learning on corpus as well (apply on corpus -transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus = cv.fit_transform(corpus)\n",
    "#vectorized_corpus its asparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x45 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 51 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_corpus\n",
    "#(4,45) matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 15)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 34)\t1\n",
      "  (0, 40)\t2\n",
      "  (0, 42)\t1\n",
      "  (0, 44)\t2\n",
      "  (0, 9)\t2\n",
      "  (0, 30)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 38)\t1\n",
      "  (0, 18)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 32)\t1\n",
      "  (0, 19)\t1\n",
      "  (1, 15)\t1\n",
      "  (1, 40)\t1\n",
      "  (1, 30)\t1\n",
      "  (1, 39)\t1\n",
      "  (1, 41)\t1\n",
      "  (1, 22)\t1\n",
      "  (1, 20)\t1\n",
      "  (1, 29)\t1\n",
      "  (1, 10)\t1\n",
      "  :\t:\n",
      "  (1, 25)\t1\n",
      "  (2, 15)\t1\n",
      "  (2, 12)\t1\n",
      "  (2, 26)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 17)\t1\n",
      "  (2, 43)\t1\n",
      "  (2, 35)\t2\n",
      "  (2, 13)\t1\n",
      "  (2, 23)\t1\n",
      "  (2, 24)\t1\n",
      "  (3, 15)\t1\n",
      "  (3, 35)\t1\n",
      "  (3, 21)\t1\n",
      "  (3, 27)\t1\n",
      "  (3, 16)\t1\n",
      "  (3, 1)\t1\n",
      "  (3, 11)\t1\n",
      "  (3, 31)\t1\n",
      "  (3, 36)\t1\n",
      "  (3, 4)\t1\n",
      "  (3, 37)\t1\n",
      "  (3, 28)\t1\n",
      "  (3, 33)\t1\n"
     ]
    }
   ],
   "source": [
    "print(vectorized_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 45)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_corpus.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 1, 1, 0, 1, 2, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 2, 0, 1, 0,\n",
       "        2],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "        0],\n",
       "       [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0],\n",
       "       [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_corpus.toarray()\n",
    "#Return a dense ndarray representation of this matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "       0], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to convert it into array or DENSE ARRAY\n",
    "vc_dense = vectorized_corpus.toarray()\n",
    "vc_dense[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_corpus.nnz\n",
    "#this function tell no of non zero elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_corpus.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_corpus.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "total=(vectorized_corpus.shape[0]*vectorized_corpus.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity=(total-vectorized_corpus.nnz)/total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7166666666666667"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparsity #see once google it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indian': 15,\n",
       " 'cricket': 8,\n",
       " 'team': 34,\n",
       " 'will': 40,\n",
       " 'wins': 42,\n",
       " 'world': 44,\n",
       " 'cup': 9,\n",
       " 'says': 30,\n",
       " 'capt': 6,\n",
       " 'virat': 38,\n",
       " 'kohli': 18,\n",
       " 'be': 5,\n",
       " 'held': 14,\n",
       " 'at': 3,\n",
       " 'sri': 32,\n",
       " 'lanka': 19,\n",
       " 'we': 39,\n",
       " 'win': 41,\n",
       " 'next': 22,\n",
       " 'lok': 20,\n",
       " 'sabha': 29,\n",
       " 'elections': 10,\n",
       " 'confident': 7,\n",
       " 'pm': 25,\n",
       " 'former': 12,\n",
       " 'president': 26,\n",
       " 'apj': 2,\n",
       " 'abdul': 0,\n",
       " 'kalam': 17,\n",
       " 'won': 43,\n",
       " 'the': 35,\n",
       " 'hearts': 13,\n",
       " 'of': 23,\n",
       " 'people': 24,\n",
       " 'movie': 21,\n",
       " 'raazi': 27,\n",
       " 'is': 16,\n",
       " 'an': 1,\n",
       " 'exciting': 11,\n",
       " 'spy': 31,\n",
       " 'thriller': 36,\n",
       " 'based': 4,\n",
       " 'upon': 37,\n",
       " 'real': 28,\n",
       " 'story': 33}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_\n",
    "#https://scikit-learn.org/stable/modules/feature_extraction.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'indian': 15, 'cricket': 8, 'team': 34, 'will': 40, 'wins': 42, 'world': 44, 'cup': 9, 'says': 30, 'capt': 6, 'virat': 38, 'kohli': 18, 'be': 5, 'held': 14, 'at': 3, 'sri': 32, 'lanka': 19, 'we': 39, 'win': 41, 'next': 22, 'lok': 20, 'sabha': 29, 'elections': 10, 'confident': 7, 'pm': 25, 'former': 12, 'president': 26, 'apj': 2, 'abdul': 0, 'kalam': 17, 'won': 43, 'the': 35, 'hearts': 13, 'of': 23, 'people': 24, 'movie': 21, 'raazi': 27, 'is': 16, 'an': 1, 'exciting': 11, 'spy': 31, 'thriller': 36, 'based': 4, 'upon': 37, 'real': 28, 'story': 33}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)# so it has 45 entries\n",
    "#dictonary which column is associated with word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0], dtype=int64)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc_dense[2]\n",
    "#to tell the mapping of words to coresssponding column number \n",
    "#or to convert array into words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.inverse_transform(vc_dense[2])\n",
    "#ordder here doesnt mATTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abdul',\n",
       " 'an',\n",
       " 'apj',\n",
       " 'at',\n",
       " 'based',\n",
       " 'be',\n",
       " 'capt',\n",
       " 'confident',\n",
       " 'cricket',\n",
       " 'cup',\n",
       " 'elections',\n",
       " 'exciting',\n",
       " 'former',\n",
       " 'hearts',\n",
       " 'held',\n",
       " 'indian',\n",
       " 'is',\n",
       " 'kalam',\n",
       " 'kohli',\n",
       " 'lanka',\n",
       " 'lok',\n",
       " 'movie',\n",
       " 'next',\n",
       " 'of',\n",
       " 'people',\n",
       " 'pm',\n",
       " 'president',\n",
       " 'raazi',\n",
       " 'real',\n",
       " 'sabha',\n",
       " 'says',\n",
       " 'spy',\n",
       " 'sri',\n",
       " 'story',\n",
       " 'team',\n",
       " 'the',\n",
       " 'thriller',\n",
       " 'upon',\n",
       " 'virat',\n",
       " 'we',\n",
       " 'will',\n",
       " 'win',\n",
       " 'wins',\n",
       " 'won',\n",
       " 'world']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names()\n",
    "#tell all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_text = [\"my name is rorbot, Indian president is a good person.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0]], dtype=int64)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.transform(my_text).toarray()\n",
    "#those words are in vocab at training time\n",
    "#its recognise them only and put it\n",
    "#to array by allocating  value to it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other ways of creating features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x95 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 101 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit_transform(corpus)\n",
    "#so we can see features are increased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indian': 31,\n",
       " 'cricket': 16,\n",
       " 'team': 69,\n",
       " 'will': 83,\n",
       " 'wins': 89,\n",
       " 'world': 93,\n",
       " 'cup': 18,\n",
       " 'says': 61,\n",
       " 'capt': 12,\n",
       " 'virat': 79,\n",
       " 'kohli': 40,\n",
       " 'be': 10,\n",
       " 'held': 29,\n",
       " 'at': 6,\n",
       " 'sri': 66,\n",
       " 'lanka': 42,\n",
       " 'indian cricket': 32,\n",
       " 'cricket team': 17,\n",
       " 'team will': 70,\n",
       " 'will wins': 86,\n",
       " 'wins world': 90,\n",
       " 'world cup': 94,\n",
       " 'cup says': 19,\n",
       " 'says capt': 62,\n",
       " 'capt virat': 13,\n",
       " 'virat kohli': 80,\n",
       " 'kohli world': 41,\n",
       " 'cup will': 20,\n",
       " 'will be': 84,\n",
       " 'be held': 11,\n",
       " 'held at': 30,\n",
       " 'at sri': 7,\n",
       " 'sri lanka': 67,\n",
       " 'we': 81,\n",
       " 'win': 87,\n",
       " 'next': 47,\n",
       " 'lok': 43,\n",
       " 'sabha': 59,\n",
       " 'elections': 21,\n",
       " 'confident': 14,\n",
       " 'pm': 52,\n",
       " 'we will': 82,\n",
       " 'will win': 85,\n",
       " 'win next': 88,\n",
       " 'next lok': 48,\n",
       " 'lok sabha': 44,\n",
       " 'sabha elections': 60,\n",
       " 'elections says': 22,\n",
       " 'says confident': 63,\n",
       " 'confident indian': 15,\n",
       " 'indian pm': 33,\n",
       " 'former': 25,\n",
       " 'president': 53,\n",
       " 'apj': 4,\n",
       " 'abdul': 0,\n",
       " 'kalam': 38,\n",
       " 'won': 91,\n",
       " 'the': 71,\n",
       " 'hearts': 27,\n",
       " 'of': 49,\n",
       " 'people': 51,\n",
       " 'former indian': 26,\n",
       " 'indian president': 34,\n",
       " 'president apj': 54,\n",
       " 'apj abdul': 5,\n",
       " 'abdul kalam': 1,\n",
       " 'kalam won': 39,\n",
       " 'won the': 92,\n",
       " 'the hearts': 72,\n",
       " 'hearts of': 28,\n",
       " 'of the': 50,\n",
       " 'the people': 74,\n",
       " 'movie': 45,\n",
       " 'raazi': 55,\n",
       " 'is': 36,\n",
       " 'an': 2,\n",
       " 'exciting': 23,\n",
       " 'spy': 64,\n",
       " 'thriller': 75,\n",
       " 'based': 8,\n",
       " 'upon': 77,\n",
       " 'real': 57,\n",
       " 'story': 68,\n",
       " 'the movie': 73,\n",
       " 'movie raazi': 46,\n",
       " 'raazi is': 56,\n",
       " 'is an': 37,\n",
       " 'an exciting': 3,\n",
       " 'exciting indian': 24,\n",
       " 'indian spy': 35,\n",
       " 'spy thriller': 65,\n",
       " 'thriller based': 76,\n",
       " 'based upon': 9,\n",
       " 'upon real': 78,\n",
       " 'real story': 58}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,2), max_features=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x40 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 46 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit_transform(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indian': 1,\n",
       " 'team': 20,\n",
       " 'will': 28,\n",
       " 'wins': 34,\n",
       " 'world': 38,\n",
       " 'cup': 0,\n",
       " 'says': 13,\n",
       " 'virat': 24,\n",
       " 'sri': 18,\n",
       " 'will wins': 31,\n",
       " 'wins world': 35,\n",
       " 'world cup': 39,\n",
       " 'says capt': 14,\n",
       " 'virat kohli': 25,\n",
       " 'will be': 29,\n",
       " 'sri lanka': 19,\n",
       " 'we': 26,\n",
       " 'win': 32,\n",
       " 'sabha': 11,\n",
       " 'pm': 4,\n",
       " 'we will': 27,\n",
       " 'will win': 30,\n",
       " 'win next': 33,\n",
       " 'sabha elections': 12,\n",
       " 'says confident': 15,\n",
       " 'president': 5,\n",
       " 'won': 36,\n",
       " 'the': 21,\n",
       " 'people': 3,\n",
       " 'president apj': 6,\n",
       " 'won the': 37,\n",
       " 'of the': 2,\n",
       " 'raazi': 7,\n",
       " 'spy': 16,\n",
       " 'real': 9,\n",
       " 'raazi is': 8,\n",
       " 'spy thriller': 17,\n",
       " 'thriller based': 22,\n",
       " 'upon real': 23,\n",
       " 'real story': 10}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TfIdf Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#http://www.tfidf.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = tfidf.fit_transform(corpus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.21138162, 0.        ,\n",
       "        0.21138162, 0.21138162, 0.        , 0.21138162, 0.42276324,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.21138162,\n",
       "        0.1103077 , 0.        , 0.        , 0.21138162, 0.21138162,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.1666557 , 0.        , 0.21138162, 0.        , 0.21138162,\n",
       "        0.        , 0.        , 0.        , 0.21138162, 0.        ,\n",
       "        0.3333114 , 0.        , 0.21138162, 0.        , 0.42276324],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.32417842, 0.        , 0.        ,\n",
       "        0.32417842, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.16916975, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.32417842, 0.        , 0.32417842, 0.        , 0.        ,\n",
       "        0.32417842, 0.        , 0.        , 0.        , 0.32417842,\n",
       "        0.25558599, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.32417842,\n",
       "        0.25558599, 0.32417842, 0.        , 0.        , 0.        ],\n",
       "       [0.29162217, 0.        , 0.29162217, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.29162217, 0.29162217, 0.        ,\n",
       "        0.15218055, 0.        , 0.29162217, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.29162217, 0.29162217,\n",
       "        0.        , 0.29162217, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.45983654, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.29162217, 0.        ],\n",
       "       [0.        , 0.28995971, 0.        , 0.        , 0.28995971,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.28995971, 0.        , 0.        , 0.        ,\n",
       "        0.151313  , 0.28995971, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.28995971, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.28995971, 0.28995971, 0.        ,\n",
       "        0.        , 0.28995971, 0.        , 0.28995971, 0.        ,\n",
       "        0.22860756, 0.28995971, 0.28995971, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc.toarray()\n",
    "#term frequency shown as its weighted so its more reliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 19)\t0.21138162089799728\n",
      "  (0, 32)\t0.21138162089799728\n",
      "  (0, 3)\t0.21138162089799728\n",
      "  (0, 14)\t0.21138162089799728\n",
      "  (0, 5)\t0.21138162089799728\n",
      "  (0, 18)\t0.21138162089799728\n",
      "  (0, 38)\t0.21138162089799728\n",
      "  (0, 6)\t0.21138162089799728\n",
      "  (0, 30)\t0.1666556978718695\n",
      "  (0, 9)\t0.42276324179599456\n",
      "  (0, 44)\t0.42276324179599456\n",
      "  (0, 42)\t0.21138162089799728\n",
      "  (0, 40)\t0.333311395743739\n",
      "  (0, 34)\t0.21138162089799728\n",
      "  (0, 8)\t0.21138162089799728\n",
      "  (0, 15)\t0.11030769881732065\n",
      "  (1, 25)\t0.32417842259348545\n",
      "  (1, 7)\t0.32417842259348545\n",
      "  (1, 10)\t0.32417842259348545\n",
      "  (1, 29)\t0.32417842259348545\n",
      "  (1, 20)\t0.32417842259348545\n",
      "  (1, 22)\t0.32417842259348545\n",
      "  (1, 41)\t0.32417842259348545\n",
      "  (1, 39)\t0.32417842259348545\n",
      "  (1, 30)\t0.255585991926846\n",
      "  :\t:\n",
      "  (1, 15)\t0.16916974924594824\n",
      "  (2, 24)\t0.29162217443775823\n",
      "  (2, 23)\t0.29162217443775823\n",
      "  (2, 13)\t0.29162217443775823\n",
      "  (2, 35)\t0.4598365438714178\n",
      "  (2, 43)\t0.29162217443775823\n",
      "  (2, 17)\t0.29162217443775823\n",
      "  (2, 0)\t0.29162217443775823\n",
      "  (2, 2)\t0.29162217443775823\n",
      "  (2, 26)\t0.29162217443775823\n",
      "  (2, 12)\t0.29162217443775823\n",
      "  (2, 15)\t0.15218054838294204\n",
      "  (3, 33)\t0.2899597081938501\n",
      "  (3, 28)\t0.2899597081938501\n",
      "  (3, 37)\t0.2899597081938501\n",
      "  (3, 4)\t0.2899597081938501\n",
      "  (3, 36)\t0.2899597081938501\n",
      "  (3, 31)\t0.2899597081938501\n",
      "  (3, 11)\t0.2899597081938501\n",
      "  (3, 1)\t0.2899597081938501\n",
      "  (3, 16)\t0.2899597081938501\n",
      "  (3, 27)\t0.2899597081938501\n",
      "  (3, 21)\t0.2899597081938501\n",
      "  (3, 35)\t0.22860756445371533\n",
      "  (3, 15)\t0.15131300453051091\n"
     ]
    }
   ],
   "source": [
    "print(vc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indian': 15,\n",
       " 'cricket': 8,\n",
       " 'team': 34,\n",
       " 'will': 40,\n",
       " 'wins': 42,\n",
       " 'world': 44,\n",
       " 'cup': 9,\n",
       " 'says': 30,\n",
       " 'capt': 6,\n",
       " 'virat': 38,\n",
       " 'kohli': 18,\n",
       " 'be': 5,\n",
       " 'held': 14,\n",
       " 'at': 3,\n",
       " 'sri': 32,\n",
       " 'lanka': 19,\n",
       " 'we': 39,\n",
       " 'win': 41,\n",
       " 'next': 22,\n",
       " 'lok': 20,\n",
       " 'sabha': 29,\n",
       " 'elections': 10,\n",
       " 'confident': 7,\n",
       " 'pm': 25,\n",
       " 'former': 12,\n",
       " 'president': 26,\n",
       " 'apj': 2,\n",
       " 'abdul': 0,\n",
       " 'kalam': 17,\n",
       " 'won': 43,\n",
       " 'the': 35,\n",
       " 'hearts': 13,\n",
       " 'of': 23,\n",
       " 'people': 24,\n",
       " 'movie': 21,\n",
       " 'raazi': 27,\n",
       " 'is': 16,\n",
       " 'an': 1,\n",
       " 'exciting': 11,\n",
       " 'spy': 31,\n",
       " 'thriller': 36,\n",
       " 'based': 4,\n",
       " 'upon': 37,\n",
       " 'real': 28,\n",
       " 'story': 33}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.vocabulary_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
